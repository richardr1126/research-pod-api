{"doi": "10.48550/arXiv.2410.21306", "date": "2024-10-25", "title": "Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges", "authors": "Farid Ariai, Gianluca Demartini", "abstract": "Natural Language Processing is revolutionizing the way legal professionals\nand laypersons operate in the legal field. The considerable potential for\nNatural Language Processing in the legal sector, especially in developing\ncomputational tools for various legal processes, has captured the interest of\nresearchers for years. This survey follows the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses framework, reviewing 148 studies, with a\nfinal selection of 127 after manual filtering. It explores foundational\nconcepts related to Natural Language Processing in the legal domain,\nillustrating the unique aspects and challenges of processing legal texts, such\nas extensive document length, complex language, and limited open legal\ndatasets. We provide an overview of Natural Language Processing tasks specific\nto legal text, such as Legal Document Summarization, legal Named Entity\nRecognition, Legal Question Answering, Legal Text Classification, and Legal\nJudgment Prediction. In the section on legal Language Models, we analyze both\ndeveloped Language Models and approaches for adapting general Language Models\nto the legal domain. Additionally, we identify 15 Open Research Challenges,\nincluding bias in Artificial Intelligence applications, the need for more\nrobust and interpretable models, and improving explainability to handle the\ncomplexities of legal language and reasoning.", "journal": ""}
{"doi": "10.48550/arXiv.2407.05399", "date": "2024-07-07", "title": "IL-TUR: Benchmark for Indian Legal Text Understanding and Reasoning", "authors": "Abhinav Joshi, Shounak Paul, Akshat Sharma, Pawan Goyal, Saptarshi Ghosh, Ashutosh Modi", "abstract": "Legal systems worldwide are inundated with exponential growth in cases and\ndocuments. There is an imminent need to develop NLP and ML techniques for\nautomatically processing and understanding legal documents to streamline the\nlegal system. However, evaluating and comparing various NLP models designed\nspecifically for the legal domain is challenging. This paper addresses this\nchallenge by proposing IL-TUR: Benchmark for Indian Legal Text Understanding\nand Reasoning. IL-TUR contains monolingual (English, Hindi) and multi-lingual\n(9 Indian languages) domain-specific tasks that address different aspects of\nthe legal system from the point of view of understanding and reasoning over\nIndian legal documents. We present baseline models (including LLM-based) for\neach task, outlining the gap between models and the ground truth. To foster\nfurther research in the legal domain, we create a leaderboard (available at:\nhttps://exploration-lab.github.io/IL-TUR/) where the research community can\nupload and compare legal text understanding systems.", "journal": ""}
{"doi": "10.48550/arXiv.2212.08204", "date": "2022-12-16", "title": "LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension", "authors": "Wenyue Hua, Yuchen Zhang, Zhe Chen, Josie Li, Melanie Weber", "abstract": "The application of Natural Language Processing (NLP) to specialized domains,\nsuch as the law, has recently received a surge of interest. As many legal\nservices rely on processing and analyzing large collections of documents,\nautomating such tasks with NLP tools emerges as a key challenge. Many popular\nlanguage models, such as BERT or RoBERTa, are general-purpose models, which\nhave limitations on processing specialized legal terminology and syntax. In\naddition, legal documents may contain specialized vocabulary from other\ndomains, such as medical terminology in personal injury text. Here, we propose\nLegalRelectra, a legal-domain language model that is trained on mixed-domain\nlegal and medical corpora. We show that our model improves over general-domain\nand single-domain medical and legal language models when processing\nmixed-domain (personal injury) text. Our training architecture implements the\nElectra framework, but utilizes Reformer instead of BERT for its generator and\ndiscriminator. We show that this improves the model's performance on processing\nlong passages and results in better long-range text comprehension.", "journal": ""}
